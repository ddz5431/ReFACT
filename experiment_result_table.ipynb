{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "from pathlib import Path\n",
    "import re\n",
    "from glob import glob \n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score,f1_score, precision_score, recall_score\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" #set if you want fast output for bertscore\n",
    "selected_prompt = \"zeroshot_no_self_consistency\"\n",
    "filter_for_prompt = True\n",
    "skip_nli_bert = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blank_filling', 'valid_invalid', 'entity_finder', 'negated_sentence', 'answer_comparison'}\n",
      "['Llama-3.2-1B-Instruct', 'Llama-3.2-3B-Instruct', 'Llama-3.3-70B-Instruct', 'gemma-3-12b-it', 'gemma-3-1b-it', 'gemma-3-27b-it', 'gemma-3-4b-it']\n",
      "['zeroshot_no_self_consistency']\n"
     ]
    }
   ],
   "source": [
    "# read refact for input data \n",
    "refact_df = pd.read_json(\"final_metadata.jsonl\", lines=True)\n",
    "refact_df = refact_df[refact_df[\"vote_valid\"] == True]\n",
    "\n",
    "experiments = []\n",
    "for path in glob(\"data/2025_submission/*/*.json\"): \n",
    "    path_obj = Path(path)\n",
    "    category = path_obj.parts[2]\n",
    "    fname = path_obj.name\n",
    "\n",
    "\n",
    "    experiment_name, model_name, prompt_type, self_consistency = fname.split(\"__\")\n",
    "    self_consistency = self_consistency.split(\".\")[0]\n",
    "    experiment_name = experiment_name.replace(\"results_\", \"\")\n",
    "\n",
    "    prompt_type = prompt_type + \"_\" + self_consistency\n",
    "\n",
    "    if prompt_type != selected_prompt and filter_for_prompt:\n",
    "        continue\n",
    "        \n",
    "    df = pd.read_json(path)\n",
    "    \n",
    "    # merge with id \n",
    "    df = df.merge(refact_df[[\"id\", \"answer\", \"transformed_answer\"]], on=\"id\", how=\"left\")\n",
    "    # append \n",
    "\n",
    "    experiments.append({\"category\": category, \n",
    "                        \"name\": experiment_name, \n",
    "                        \"model\": model_name, \n",
    "                        \"prompt_type\": prompt_type,\n",
    "                        \"df\": df})\n",
    "    \n",
    "\n",
    "# [(e[\"category\"], e[\"name\"], e[\"model\"]) for e in experiments]\n",
    "\n",
    "experiments = sorted(experiments, key=lambda e: (e[\"category\"], e[\"name\"], e[\"model\"]))\n",
    "\n",
    "experiment_names = set(e[\"name\"] for e in experiments)\n",
    "model_names = sorted(set(e[\"model\"] for e in experiments))\n",
    "prompt_types = sorted(set(e[\"prompt_type\"] for e in experiments),key = lambda x: {\"zeroshot\":0,\"fewshot\":1,\"zeroshotcot\":2,\"cot\":3}[x.split(\"_\")[0]])\n",
    "print(experiment_names)\n",
    "print(model_names)  \n",
    "print(prompt_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BertScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "print(device)\n",
    "\n",
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "def calculate_bert_score_for_string(sequence1,sequence2):\n",
    "    if skip_nli_bert:\n",
    "        return 0 \n",
    "    # Tokenize input sequences and get token embeddings\n",
    "    \n",
    "    results = bertscore.compute(predictions=[sequence1], references=[sequence2], lang=\"en\",device=device)\n",
    "    \n",
    "    return results[\"f1\"][0]\n",
    "\n",
    "\n",
    "def bert_score_for_lists(list1,list2):\n",
    "    if skip_nli_bert:\n",
    "        return 0 \n",
    "    if  len(list1) != len(list2):\n",
    "        return 0\n",
    "    if len(list1)==0:\n",
    "        return 0\n",
    "    score = 0  \n",
    "    try: \n",
    "        results = bertscore.compute(predictions=list1, references=list2, lang=\"en\",device=device)\n",
    "        score = results[\"f1\"]\n",
    "        score = sum(score)/len(list1)\n",
    "    except: \n",
    "        return 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NLIClassifier:\n",
    "    \"\"\"\n",
    "    Classify the relationship between two texts.\n",
    "    Classes have to be provided when initializing the class.\n",
    "\n",
    "    For example if two texts are contradicting, provide classes as [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    \"\"\"\n",
    "\n",
    "    model = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        classes = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    ):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model)\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "        self.classifier = AutoModelForSequenceClassification.from_pretrained(self.model).to(self.device)\n",
    "        self.classes = classes\n",
    "\n",
    "    def infer(self, premise: str, hypothesis: str):\n",
    "        input_ = self.tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\").to(self.classifier.device)\n",
    "        output = self.classifier(input_[\"input_ids\"])\n",
    "        prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "\n",
    "        return {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, self.classes)}\n",
    "\n",
    "    def infer_batch(self, premises, hypothesises):\n",
    "        input_ = self.tokenizer(premises, hypothesises, padding=True, truncation=True, return_tensors=\"pt\").to(\n",
    "            self.classifier.device\n",
    "        )\n",
    "        output = self.classifier(input_[\"input_ids\"])\n",
    "        predictions = torch.softmax(output[\"logits\"], -1).tolist()\n",
    "        return [\n",
    "            {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, self.classes)}\n",
    "            for prediction in predictions\n",
    "        ]\n",
    "\n",
    "\n",
    "nli_model =  NLIClassifier()\n",
    "\n",
    "\n",
    "def nli_scores_for_lists(premises,hypotheses):\n",
    "    if skip_nli_bert:\n",
    "        return 0 \n",
    "    if len(premises) != len(hypotheses):\n",
    "        return 0\n",
    "    if len(premises)==0:\n",
    "        return 0\n",
    "    scores = nli_model.infer_batch(premises, hypotheses)\n",
    "    # filter out entailment\n",
    "    \n",
    "    #avg all 3 scores \n",
    "\n",
    "    avg_entailment = 0\n",
    "    avg_neutral = 0\n",
    "    avg_contradiction = 0\n",
    "\n",
    "    for score in scores:\n",
    "        avg_entailment += score[\"entailment\"]\n",
    "        avg_neutral += score[\"neutral\"]\n",
    "        avg_contradiction += score[\"contradiction\"]\n",
    "    avg_entailment /= len(scores)\n",
    "    avg_neutral /= len(scores)\n",
    "    avg_contradiction /= len(scores)\n",
    "\n",
    "    return avg_entailment, avg_neutral, avg_contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU: 0.60\n"
     ]
    }
   ],
   "source": [
    "def get_indices(text, extracts, used_preds=None):\n",
    "    \"\"\"Convert a list of string spans to a set of character indices in the text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        print(\"failed for text\", text)\n",
    "        return set()\n",
    "    \n",
    "    if not isinstance(extracts, list):\n",
    "        print(\"failed for text\", text)\n",
    "        return set()\n",
    "    \n",
    "    char_indices = set()\n",
    "    for extract in extracts:\n",
    "        if not isinstance(extract, str):\n",
    "            print(\"failed for extract\", extract)\n",
    "            continue\n",
    "        start = text.lower().find(extract.lower().strip())\n",
    "        if start != -1:\n",
    "            char_indices.update(range(start, start + len(extract)))\n",
    "    return char_indices\n",
    "\n",
    "def calculate_iou_char_level(text, gts, preds):\n",
    "    pred_indices= get_indices(text, preds, used_preds=True)\n",
    "    gt_indices = get_indices(text, gts, used_preds=False)\n",
    "    \n",
    "    intersection = pred_indices & gt_indices\n",
    "    union = pred_indices | gt_indices\n",
    "    \n",
    "    return len(intersection) / len(union) if union else 0.0\n",
    "\n",
    "text = \"Barack Obama was born in Hawaii. He was elected president in 2008.\"\n",
    "predicted = [\"Barack Obama\", \"president\"]\n",
    "ground_truth = [\"Barack Obama\", \"born in Hawaii\", \"president\"]\n",
    "\n",
    "iou = calculate_iou_char_level(text, ground_truth, predicted)\n",
    "print(f\"IoU: {iou:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### valid_invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-3.2-1B-Instruct + zeroshot_no_self_consistency: |  Sklearn Accuracy: 0.49 | Precision: 0.48 | Recall: 0.33 | F1 Score: 0.39 | False F1 Score: 0.56 | Postprocessing Errors: 154 |\n",
      "Llama-3.2-3B-Instruct + zeroshot_no_self_consistency: |  Sklearn Accuracy: 0.52 | Precision: 0.54 | Recall: 0.25 | F1 Score: 0.34 | False F1 Score: 0.62 | Postprocessing Errors: 50 |\n",
      "Llama-3.3-70B-Instruct + zeroshot_no_self_consistency: |  Sklearn Accuracy: 0.67 | Precision: 0.62 | Recall: 0.90 | F1 Score: 0.73 | False F1 Score: 0.57 | Postprocessing Errors: 0 |\n",
      "gemma-3-12b-it + zeroshot_no_self_consistency: |  Sklearn Accuracy: 0.65 | Precision: 0.68 | Recall: 0.58 | F1 Score: 0.63 | False F1 Score: 0.68 | Postprocessing Errors: 0 |\n",
      "gemma-3-1b-it + zeroshot_no_self_consistency: |  Sklearn Accuracy: 0.51 | Precision: 0.51 | Recall: 0.56 | F1 Score: 0.53 | False F1 Score: 0.48 | Postprocessing Errors: 0 |\n",
      "gemma-3-27b-it + zeroshot_no_self_consistency: |  Sklearn Accuracy: 0.71 | Precision: 0.69 | Recall: 0.75 | F1 Score: 0.72 | False F1 Score: 0.69 | Postprocessing Errors: 0 |\n",
      "gemma-3-4b-it + zeroshot_no_self_consistency: |  Sklearn Accuracy: 0.58 | Precision: 0.58 | Recall: 0.62 | F1 Score: 0.60 | False F1 Score: 0.57 | Postprocessing Errors: 0 |\n"
     ]
    }
   ],
   "source": [
    "def error_detection_extract(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    #pattern = r\"\\**\\s*Final\\s+Verdict\\s*\\**:?\\s*\\**(True|False)\\**\"\n",
    "    pattern = r\"\\**\\s*Final\\s+Verdict.*?(True|False)\"\n",
    "    match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).lower()\n",
    "    \n",
    "    #no final verdict found \n",
    "    #--> go for last bool \n",
    "    pattern = r\"(True|False)\"\n",
    "    match = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "    if match:\n",
    "        return match[-1].lower()\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_pred_answer(response): \n",
    "    regex_response = error_detection_extract(response)\n",
    "    if regex_response is None: \n",
    "        #print(response)\n",
    "        return \"Postprocessing Error\"\n",
    "    return regex_response.split(\"Final Verdict:\")[-1].strip()\n",
    "\n",
    "valid_invalid_experiments = [e for e in experiments if e[\"name\"] == \"valid_invalid\"]\n",
    "\n",
    "\n",
    "f1_scores = {model_name: {prompt_type: 0 for prompt_type in prompt_types} for model_name in model_names}\n",
    "\n",
    "for experiment in valid_invalid_experiments:\n",
    "        model_name = experiment[\"model\"]\n",
    "        prompt_type = experiment[\"prompt_type\"]\n",
    "        df = experiment[\"df\"]\n",
    "\n",
    "        #if not (model_name == \"Llama-3.2-1B-Instruct\" and prompt_type == \"fewshot_no_self_consistency\"):\n",
    "        #    continue\n",
    "\n",
    "        df[\"verdict_original\"] = df[\"output_original\"].apply(get_pred_answer)\n",
    "        df[\"verdict_transformed\"] = df[\"output_transformed\"].apply(get_pred_answer)\n",
    "\n",
    "        #get number of postprocessing errors\n",
    "        num_errors = df[\"verdict_original\"].apply(lambda x: x == \"Postprocessing Error\").sum()\n",
    "        num_errors += df[\"verdict_transformed\"].apply(lambda x: x == \"Postprocessing Error\").sum()\n",
    "\n",
    "        y_preds_correct_input = list(df[\"verdict_original\"].apply(lambda x: True if x == \"true\" else (False if x == \"false\" else False)))\n",
    "        y_preds_wrong_input = list(df[\"verdict_transformed\"].apply(lambda x: True if x == \"true\" else (False if x == \"false\" else True)))\n",
    "        \n",
    "        # Filter out None values\n",
    "        #y_preds_correct_input = [x for x in y_preds_correct_input if x is not None]\n",
    "        #y_preds_wrong_input = [x for x in y_preds_wrong_input if x is not None]\n",
    "\n",
    "        y_pred = np.concatenate((y_preds_correct_input, y_preds_wrong_input))\n",
    "        y_true = np.array([True]*len(y_preds_correct_input)+[False]*len(y_preds_wrong_input))\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        false_f1 = f1_score(y_true, y_pred, pos_label=False)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"{model_name} + {prompt_type}: |  Sklearn Accuracy: {accuracy:.2f} | Precision: {precision:.2f} | Recall: {recall:.2f} | F1 Score: {f1:.2f} | False F1 Score: {false_f1:.2f} | Postprocessing Errors: {num_errors} |\")\n",
    "        f1_scores[model_name][prompt_type] =  (f1 + false_f1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeroshot_no_self_consistency    0.584289\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#best prompt f1 score \n",
    "f1_df = pd.DataFrame(f1_scores).T\n",
    "print(f1_df.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### answer_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-3.2-1B-Instruct + zeroshot_no_self_consistency: | Postprocessing Errors: 0 | Sklearn Accuracy: 0.50 | Precision: 0.32 | Recall: 0.33 | F1 Score: 0.27 | False F1 Score: 0.27 | \n",
      "Llama-3.2-3B-Instruct + zeroshot_no_self_consistency: | Postprocessing Errors: 0 | Sklearn Accuracy: 0.48 | Precision: 0.33 | Recall: 0.32 | F1 Score: 0.29 | False F1 Score: 0.29 | \n",
      "Llama-3.3-70B-Instruct + zeroshot_no_self_consistency: | Postprocessing Errors: 0 | Sklearn Accuracy: 0.50 | Precision: 0.52 | Recall: 0.50 | F1 Score: 0.39 | False F1 Score: 0.39 | \n",
      "gemma-3-12b-it + zeroshot_no_self_consistency: | Postprocessing Errors: 0 | Sklearn Accuracy: 0.71 | Precision: 0.48 | Recall: 0.48 | F1 Score: 0.48 | False F1 Score: 0.48 | \n",
      "gemma-3-1b-it + zeroshot_no_self_consistency: | Postprocessing Errors: 0 | Sklearn Accuracy: 0.53 | Precision: 0.36 | Recall: 0.35 | F1 Score: 0.34 | False F1 Score: 0.34 | \n",
      "gemma-3-27b-it + zeroshot_no_self_consistency: | Postprocessing Errors: 0 | Sklearn Accuracy: 0.56 | Precision: 0.55 | Recall: 0.55 | F1 Score: 0.54 | False F1 Score: 0.54 | \n",
      "gemma-3-4b-it + zeroshot_no_self_consistency: | Postprocessing Errors: 0 | Sklearn Accuracy: 0.52 | Precision: 0.37 | Recall: 0.36 | F1 Score: 0.33 | False F1 Score: 0.33 | \n"
     ]
    }
   ],
   "source": [
    "def answer_comparison_extract(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    pattern = r\"(?:answer\\s*)?([ab]).*?correct\"\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    \n",
    "    pattern = r\"\\**\\s*Final\\s+Verdict.*?(A|B)\"\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_pred_answer(response): \n",
    "    regex_response = answer_comparison_extract(response)\n",
    "    #print(regex_response)\n",
    "    if regex_response is None: \n",
    "       #print(response)\n",
    "        return \"Postprocessing Error\"\n",
    "    return regex_response\n",
    "\n",
    "def is_correct(row): \n",
    "    if row[\"pred\"] == \"A\" and row[\"ground_truth\"][0] == \"Original\": \n",
    "        return True\n",
    "    elif row[\"pred\"] == \"B\" and row[\"ground_truth\"][1] == \"Original\": \n",
    "        return True\n",
    "    else: \n",
    "        return False \n",
    "\n",
    "def get_ytrue(ground_truth): \n",
    "    if ground_truth[0] == \"Original\": \n",
    "        return \"A\"\n",
    "    elif ground_truth[1] == \"Original\":\n",
    "        return \"B\"\n",
    "    else:\n",
    "        return \"C\"\n",
    "    \n",
    "answer_comparison_experiments = [e for e in experiments if e[\"name\"] == \"answer_comparison\"]\n",
    "\n",
    "for experiment in answer_comparison_experiments:\n",
    "    model_name = experiment[\"model\"]\n",
    "    prompt_type = experiment[\"prompt_type\"]\n",
    "    df = experiment[\"df\"]\n",
    "\n",
    "    df[\"pred\"] = df[\"output\"].apply(get_pred_answer)\n",
    "    y_pred = list(df[\"pred\"])\n",
    "    y_true = list(df[\"ground_truth\"].apply(lambda x: get_ytrue(x)))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=\"macro\",zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average=\"macro\",zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\",zero_division=0)\n",
    "    false_f1 = f1_score(y_true, y_pred, average=\"macro\",zero_division=0)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"{model_name} + {prompt_type}: | Postprocessing Errors: {num_errors} | Sklearn Accuracy: {accuracy:.2f} | Precision: {precision:.2f} | Recall: {recall:.2f} | F1 Score: {f1:.2f} | False F1 Score: {false_f1:.2f} | \")\n",
    "    f1_scores[model_name][prompt_type] =  (f1 + false_f1) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blank-filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-3.2-1B-Instruct: length_mismatches: 555 | accuracy: 0.56%, bert_mean:0.0%,\n",
      "Llama-3.2-3B-Instruct: length_mismatches: 328 | accuracy: 1.68%, bert_mean:0.0%,\n",
      "Llama-3.3-70B-Instruct: length_mismatches: 16 | accuracy: 15.70%, bert_mean:0.0%,\n",
      "gemma-3-12b-it: length_mismatches: 16 | accuracy: 18.90%, bert_mean:0.0%,\n",
      "gemma-3-1b-it: length_mismatches: 265 | accuracy: 0.00%, bert_mean:0.0%,\n",
      "gemma-3-27b-it: length_mismatches: 16 | accuracy: 24.43%, bert_mean:0.0%,\n",
      "gemma-3-4b-it: length_mismatches: 118 | accuracy: 9.74%, bert_mean:0.0%,\n"
     ]
    }
   ],
   "source": [
    "def get_pred_entities(response):\n",
    "\n",
    "    pattern = r\"Replacements?\\s*:?\"\n",
    "    match = re.search(pattern, response, re.IGNORECASE | re.DOTALL)\n",
    "    if match:\n",
    "        response = response[match.end():]\n",
    "    \n",
    "    # enumeration \n",
    "    # find all preceeding numbers and remove them then split by new line\n",
    "    pattern = r\"(\\d+)\\s*\\.+\\s*\"\n",
    "    match = re.findall(pattern, response, re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        response = re.sub(pattern, \"\", response)\n",
    "\n",
    "    # split by new line groups \n",
    "    return [s.strip() for s in response.split(\"\\n\") if s.strip()]\n",
    "    \n",
    "\n",
    "def get_accuracy(row):\n",
    "    set1 = set(row[\"original_entities\"])\n",
    "    set2 = set(row[\"pred_entities\"])\n",
    "    intersection = set1.intersection(set2) \n",
    "    \n",
    "    return  len(intersection) / len(set1)\n",
    "    \n",
    "blank_filling_experiments = [e for e in experiments if e[\"name\"] == \"blank_filling\"]\n",
    "\n",
    "for experiment in blank_filling_experiments:\n",
    "    model_name = experiment[\"model\"]\n",
    "    prompt_type = experiment[\"prompt_type\"]\n",
    "    df = experiment[\"df\"]\n",
    "    df[\"pred_entities\"] = df[\"response\"].apply(lambda x: get_pred_entities(x))\n",
    "\n",
    "    #print(df[[\"pred_entities\", \"original_entities\"]].head(10))\n",
    "\n",
    "    length_mismatches = df.apply(lambda x: len(x[\"pred_entities\"]) != len(x[\"original_entities\"]), axis=1).sum()\n",
    "    df[\"is_correct\"] = df.apply(lambda row: get_accuracy(row), axis=1)\n",
    "    #bert_score\n",
    "    df[\"bert_score\"] = df.apply(lambda x: bert_score_for_lists(x[\"original_entities\"],x[\"pred_entities\"]),axis=1)\n",
    "    bert_score_mean = df[\"bert_score\"].mean() *100\n",
    "    \n",
    "    print(f\"{model_name}: length_mismatches: {length_mismatches} | accuracy: {df.is_correct.mean()*100:.2f}%, bert_mean:{bert_score_mean}%,\")\n",
    "    \n",
    "\n",
    "# for i in range(2, 5): \n",
    "#     pprint(experiments[\"blank_filling\"]['Meta-Llama-3-70B-Instruct'].iloc[i].to_dict())\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entity-finder_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-3.2-1B-Instruct: length mismatches 519 accuracy 7.76%, iou_mean: 4.411544239838997 bert_mean:0.0%, avg_nli_scores: 0 for num_samples 556\n",
      "Llama-3.2-3B-Instruct: length mismatches 482 accuracy 4.23%, iou_mean: 7.806044707581488 bert_mean:0.0%, avg_nli_scores: 0 for num_samples 556\n",
      "Llama-3.3-70B-Instruct: length mismatches 467 accuracy 12.86%, iou_mean: 23.736299228877407 bert_mean:0.0%, avg_nli_scores: 0 for num_samples 556\n",
      "gemma-3-12b-it: length mismatches 442 accuracy 45.82%, iou_mean: 25.92091778797702 bert_mean:0.0%, avg_nli_scores: 0 for num_samples 556\n",
      "gemma-3-1b-it: length mismatches 223 accuracy 9.61%, iou_mean: 9.580185171765605 bert_mean:0.0%, avg_nli_scores: 0 for num_samples 556\n",
      "gemma-3-27b-it: length mismatches 390 accuracy 45.81%, iou_mean: 28.69548261722897 bert_mean:0.0%, avg_nli_scores: 0 for num_samples 556\n",
      "gemma-3-4b-it: length mismatches 401 accuracy 37.86%, iou_mean: 24.676327344859256 bert_mean:0.0%, avg_nli_scores: 0 for num_samples 556\n"
     ]
    }
   ],
   "source": [
    "def extract_entities(response):\n",
    "\n",
    "    #print(response)\n",
    "\n",
    "    pattern = r\"(Wrong)?\\s*(Entity|Entities)+\\s*:\"\n",
    "    match = re.search(pattern, response, re.IGNORECASE | re.DOTALL)\n",
    "    if match:\n",
    "        response = response[match.end():].strip()\n",
    "    elif response.find(\":\\n\"): \n",
    "        response = response.split(\":\\n\")[-1]\n",
    "    \n",
    "\n",
    "    # enumeration \n",
    "    # find all preceeding numbers and remove them then split by new line\n",
    "    pattern = r\"\\d+\\s*\\.+\\s*\"\n",
    "    match = re.findall(pattern, response, re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        response = re.sub(pattern, \"\", response)\n",
    "    \n",
    "    entities = response.split(\"\\n\")\n",
    "    entities = list(filter(lambda var: var!=\"\\n\" and len(var)!=0 and var!=\" \" and var!=\"\",entities))\n",
    "    return entities\n",
    "\n",
    "def evaluate_find_entities(data):\n",
    "    #exact match\n",
    "    value_counts = (data[\"suggested_entities\"] == data[\"pred\"]).value_counts()\n",
    "    exact_match_accuracy =  value_counts[True] /len(data) *100\n",
    "    \n",
    "    #bert_score\n",
    "    data[\"sentence_similarity\"] = data.apply(lambda x: bert_score_for_lists(x[\"suggested_entities\"],x[\"pred\"]),axis=1)\n",
    "    bert_score_mean =  data[\"sentence_similarity\"].mean() *100\n",
    "    \n",
    "    return exact_match_accuracy, bert_score_mean\n",
    "def get_accuracy(row):\n",
    "    try:\n",
    "        set1 = set(row[\"suggested_entities\"])\n",
    "        set2 = set(row[\"pred\"])\n",
    "        intersection = set1.intersection(set2) \n",
    "\n",
    "        return  len(intersection) / len(set1)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "entity_finder_experiments = [e for e in experiments if e[\"name\"] == \"entity_finder\"]\n",
    "\n",
    "for experiment in entity_finder_experiments:\n",
    "    model_name = experiment[\"model\"]\n",
    "    prompt_type = experiment[\"prompt_type\"]\n",
    "    df = experiment[\"df\"]\n",
    "\n",
    "    df[\"pred\"] = df[\"predicte_wrong_entities\"].apply(lambda x: extract_entities(x))\n",
    "    df[\"suggested_entities\"] = df[\"suggested_entities\"].apply(lambda x: x[0] if isinstance(x[0], list) else x)\n",
    "    length_mismatches = df.apply(lambda x: len(x[\"pred\"]) != len(x[\"suggested_entities\"]), axis=1).sum()\n",
    "\n",
    "    #exact_match_accuracy, bert_score_mean = evaluate_find_entities(df)\n",
    "    exact_match_accuracy = df.apply(lambda row: get_accuracy(row), axis=1).mean()*100\n",
    "\n",
    "    iou_mean = df.apply(lambda row: calculate_iou_char_level(row[\"transformed_answer\"], row[\"suggested_entities\"], row[\"pred\"]), axis=1).mean() * 100\n",
    "    #print(df.head(5)[[\"suggested_entities\",\"pred\"]])\n",
    "    \n",
    "    df[\"concatenated_suggested_entities\"] = df[\"suggested_entities\"].apply(lambda x: \" \".join(x))\n",
    "    df[\"concatenated_pred\"] = df[\"pred\"].apply(lambda x: \" \".join(x))\n",
    "    nli_scores = nli_scores_for_lists(df[\"concatenated_suggested_entities\"].tolist(), df[\"concatenated_pred\"].tolist())\n",
    "\n",
    "    print(f\"{model_name}: length mismatches {length_mismatches} accuracy {exact_match_accuracy:.2f}%, iou_mean: {iou_mean} bert_mean:{bert_score_mean}%, avg_nli_scores: {nli_scores} for num_samples {len(df)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### negated_sentence_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-3.2-1B-Instruct: 0.00%, iou_mean 0.05459839844697889 bert_mean:0.0%, nli_scores: 0 for num_samples 634\n",
      "Llama-3.2-3B-Instruct: 2.37%, iou_mean 1.9827478657612858 bert_mean:0.0%, nli_scores: 0 for num_samples 634\n",
      "Llama-3.3-70B-Instruct: 69.24%, iou_mean 60.925416526719765 bert_mean:0.0%, nli_scores: 0 for num_samples 634\n",
      "gemma-3-12b-it: 44.32%, iou_mean 44.51313047820609 bert_mean:0.0%, nli_scores: 0 for num_samples 634\n",
      "gemma-3-1b-it: 13.25%, iou_mean 12.881552517880573 bert_mean:0.0%, nli_scores: 0 for num_samples 634\n",
      "gemma-3-27b-it: 60.73%, iou_mean 56.38200495427698 bert_mean:0.0%, nli_scores: 0 for num_samples 634\n",
      "gemma-3-4b-it: 36.28%, iou_mean 33.307192390593734 bert_mean:0.0%, nli_scores: 0 for num_samples 634\n"
     ]
    }
   ],
   "source": [
    "def extract_sentence(response):\n",
    "\n",
    "    pattern = r\"(Wrong)?\\s*(Sentence)+\\s*:?\"\n",
    "    match = re.search(pattern, response, re.IGNORECASE | re.DOTALL)\n",
    "    if match:\n",
    "        response = response[match.end():].strip()\n",
    "    \n",
    "    elif response.find(\":\\n\"): \n",
    "        response = response.split(\":\\n\")[-1].strip()\n",
    "\n",
    "    sentence = response.replace(\"\\n\\n\",\"\").replace(\"\\n\",\"\").strip()\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def evaluate_find_sentence(data):\n",
    "    \n",
    "    data[\"predicted_sentence\"] = data[\"predicted_sentence\"].apply(lambda x: extract_sentence(x))\n",
    "    \n",
    "    #exact match\n",
    "    value_counts = (data[\"transformed_sentence\"] == data[\"predicted_sentence\"]).value_counts()\n",
    "    exact_match_accuracy =  value_counts.get(True, 0) /len(data) *100\n",
    "    \n",
    "    #bert_score\n",
    "    data[\"sentence_similarity\"] = data.apply(lambda x: calculate_bert_score_for_string(x[\"transformed_sentence\"],x[\"predicted_sentence\"]),axis=1)\n",
    "    bert_score_mean = data[\"sentence_similarity\"].mean() *100\n",
    "    \n",
    "    nli_scores = nli_scores_for_lists(data[\"transformed_sentence\"].tolist(), data[\"predicted_sentence\"].tolist())\n",
    "    return exact_match_accuracy, bert_score_mean, nli_scores\n",
    " \n",
    "negated_sentence_experiments = [e for e in experiments if e[\"name\"] == \"negated_sentence\"]\n",
    "\n",
    "for experiment in negated_sentence_experiments:\n",
    "    model_name = experiment[\"model\"]\n",
    "    prompt_type = experiment[\"prompt_type\"]\n",
    "    df = experiment[\"df\"]\n",
    "\n",
    "    exact_match_accuracy, bert_score_mean, nli_scores = evaluate_find_sentence(df)\n",
    "    iou_mean = df.apply(lambda row: calculate_iou_char_level(row[\"transformed_answer\"], [row[\"transformed_sentence\"]], [row[\"predicted_sentence\"]]), axis=1).mean() * 100\n",
    "    print(f\"{model_name}: {exact_match_accuracy:.2f}%, iou_mean {iou_mean} bert_mean:{bert_score_mean}%, nli_scores: {nli_scores} for num_samples {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline comparison \n",
    "\n",
    "#get metadata df \n",
    "\n",
    "metadata_path = \"data/final_metadata.jsonl\"\n",
    "metadata_df = pd.read_json(metadata_path, lines=True)\n",
    "\n",
    "#only_swaps \n",
    "metadata_df = metadata_df[metadata_df[\"tag_type\"]==\"swap\"]\n",
    "# Ensure intermediate_results exists and is subscriptable\n",
    "metadata_df = metadata_df[metadata_df[\"intermediate_results\"].apply(lambda x: isinstance(x, dict) and \"new_entities\" in x)]\n",
    "\n",
    "#get metadat for entities\n",
    "metadata_df[\"original_entities\"] = metadata_df.apply(lambda row: row[\"intermediate_results\"][\"entities\"],axis=1)\n",
    "metadata_df[\"transformed_entities\"] = metadata_df.apply(lambda row: row[\"intermediate_results\"][\"new_entities\"],axis=1)\n",
    "\n",
    "print(len(metadata_df))\n",
    "metadata_df[\"sentence_similarity\"] = metadata_df.apply(lambda x: bert_score_for_lists(x[\"original_entities\"],x[\"transformed_entities\"]),axis=1)\n",
    "bert_score_mean = metadata_df[\"sentence_similarity\"].mean() *100\n",
    "print(f\"Bert Score Mean of Original and Transformed Entities {bert_score_mean:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_the_mouse-sq17Cc7w",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
